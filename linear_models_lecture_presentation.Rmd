---
title: "Linear regression models with linear algebra and R"
author: "Juho Kopra"
institute: |
  University of Eastern Finland, School of Computing
  Partly based on material of [https://github.com/genomicsclass/labs](https://github.com/genomicsclass/labs)
date: "`r Sys.Date()`"
fontsize: 8pt
output: 
  beamer_presentation:
    slide_level: 4
    incremental: FALSE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=5)
```

<!-- Idea for data: use COVID-vaccination data cube https://thl.fi/fi/tilastot-ja-data/aineistot-ja-palvelut/avoin-data -->
## 1. Scatter plot and one linear predictor

```{r message=FALSE, echo=FALSE, eval=F}
#install.packages("UsingR")
data(father.son,package="UsingR")
x=father.son$fheight
y=father.son$sheight
```

```{r galton_data, fig.cap="Scatterplot and a linear model with one predictor", out.width="50%", fig.align = 'center', echo=F, eval=F}
m1 <- lm(sheight ~ fheight,data=father.son)
plot(x,y,xlab="X",ylab="Y")
abline(coef(m1))
```
Let's consider following data set, and draw a scatter plot out of it.
```{r sim_data, echo=F}
set.seed(40)
n <- 8
x <- 0+runif(n,0,10)
y <- 4.0 + 5.3*x + rnorm(n,sd=5)
dat <- data.frame(x=round(x,2),y=round(y))
#knitr::kable(dat)f
```

The scatterplot becomes
```{r fig.cap="Scatterplot and a linear model with one predictor.", out.width="80%", fig.align = 'center', echo=F}
plot(dat$x,dat$y,xlim=c(0,10),xlab="x",ylab="y")
m1 <- lm(y~x,data=dat)
abline(coef(m1))
```

The coefficients of a linear regression fitted to that data set are $\beta_0=8.01$ and $\beta_1=3.90$.
 
<!-- TODO: explain how we can fit a linear model to a scatter plot so that we minimise the sum of squared differences -->

### Definition

 - Importantly, a linear regression model with one variable is a statistical model which can be formulated as
\begin{equation}
Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i \label{simple-regression}
\end{equation}
and where $i = 1,\dots,n$ and $\varepsilon_i \sim N(0,\sigma^2)$.

 - There the following terminology takes place
    - $Y_i$ is dependent
    - $\beta_0$ is an intercept
    - $\beta_1$ is a slope
    - $\beta_0$ and $\beta_1$ are regression coefficients or model parameters
    - $\varepsilon_i$ is an error term
    - $x_i$ is the value of predictor aka independent aka covariate aka regressor
    
#### {-}
 - In equation $\eqref{simple-regression}$ we did not write estimated values of parameters down to the equation but used $\beta$-coefficients instead. When we write it down with $\beta$-coefficients we call it general form of linear model.

 - Linear model with multiple predictors $X_1, X_2, \ldots, X_k$ it can be written as
 \begin{equation}
Y_i = \beta_0 + \beta_1 x_{i1} + \cdots \beta_k x_{ik}+ \varepsilon_i \label{multiple-regression}
\end{equation}
and where $i = 1,\dots,n$ and $\varepsilon_i \sim N(0,\sigma^2)$.

### The fitting of linear model

 - We want to minimize the sum of squares of error terms $\varepsilon_i^2$, which can be written as $\varepsilon_i = Y_i - (\beta_0 + \beta_1 X_i)$. Note that error terms are in the same direction with $Y$-axis (and thus the same direction with $Y_i$).
 - We will not go into technical details of minimization here.
 - Using R we can fit the above model as follows. Assume that we have already read in data into object `dat`. We use `lm` function to fit the **l**inear **m**odel. The resulting calculation we store into object named `m1`. The data object `dat` has columns named `x` and `y` but the names could be something else as well. The first argument of `lm` is formula, which is special type in R. Formula is handy for fitting many types of models. On the left hand side of the formula one writes the dependent (e.g. what is $Y_i$), then tilde (~) and then predictor variable(s). For multiple predictors one need to put a plus sign (`+`) between the predictors.
```{r}
m1 <- lm(y ~ x, data=dat)
m1
```

\newpage
## 2. Properties of linear models and interpretation

### Properties of linear models

To have an interpretation for $\beta_0$ and $\beta_1$ we note that:

Expected value of $Y_i$ is if we consider $x_i$ being known and fixed:
\[
\begin{aligned}
E(Y_i|x_i) &= E(\beta_0 + \beta_1 x_i + \varepsilon_i) \\
&= E(\beta_0) + E(\beta_1 x_i) + E(\varepsilon_i) \\
&= E(\beta_0) + x_i E(\beta_1) + E(\varepsilon_i) \\
&= \beta_0 +  \beta_1 x_i
\end{aligned}
\]
because $x_i$ is constant and $E(\varepsilon_i) = 0$.

Shortly put, the above calculation gives $E(Y_i|x_i) = \beta_0 +  \beta_1 x_i$, which is called a systematic part of linear model. 

#### {-}

 - If we replace the $\beta$-notation with their estimates, then we can use that equation to calculate fitted values:
\[\hat{Y_i} = \hat{\beta_0} + \hat{\beta_1}x_i\]
Fitted values are calculated for the same $x_i$ values which were in the original data.
    ```{r}
    fitted(m1)
    ```
 - Prediction can be obtained by the same equation but using any value. Here we predict new values using `x=10` and `x=11`.
    ```{r}
    predict(m1,newdata = data.frame(x=c(10,11)))
    ```

### Interpretation

 - We want to interpret what happens in the population of our study. That is why we use equation of expected value of $Y_i$, that is $E(Y_i|x_i) = \beta_0 +  \beta_1 x_i$ to give interpretation.

 - An interpretation of $\beta_1$ becomes available as
\[
\begin{aligned}
E(Y_i|x_i+1) - E(Y_i|x_i) &=  (\beta_0 +  \beta_1 (x_i+1)) - (\beta_0 +  \beta_1 x_i)\\
&= \beta_1 (x_i+1) - \beta_1 x_i \\
&= \beta_1 x_i+\beta_1\cdot1 - \beta_1 x_i \\
&= \beta_1
\end{aligned}
\]

 - Thus, if e.g. $\beta_1 = `r coef(m1)[2]`$ then the expected value of $Y_i$ increases by $`r coef(m1)[2]`$ units of y if $x_i$ becomes increased with 1 unit of x.
 - An interpretation of intercept term $\beta_0$ becomes available if we set $x_i = 0$ since that removes $\beta_1$ from the equation. Thus $E(Y_i|x_i=0) = \beta_0 +  \beta_1 \cdot 0 = \beta_0$.

\newpage

## 3. Categorical predictors

The predictors may as well be categorical ones. For example, we can model the mice weights where mice have two groups in a study. We are used to do that with a t-test but here we formulate it using linear regression. The results are the very same as with t-test!

```{r fig.cap="Mice weight with jitter. Example borrowed from Mike Love.", out.width="80%", fig.align = 'center', echo=F}
dat <- read.csv("femaleMiceWeights.csv")
stripchart(Bodyweight ~ Diet, data=dat, vertical=TRUE,
method="jitter", pch=1, main="Mice weights")
```

```{r}
m1 <- lm(Bodyweight ~ Diet, data=dat)
```
```{r}
summary(m1)
```

Above, the `(Intercept)` represents the average of group `chow` that is `r coef(m1)[1]` and the `Diethf` is parameter comparing the differences of means between the two groups. Thus, p-value $p=0.052$ tells us that these two groups do not differ from each other.

```{r t-test-comparison}
t.test(Bodyweight ~ Diet, data=dat, var.equal=T)
```


<!-- TODO: dummy aka indicator variables help to understand how categorical predictors work. -->

#### Dummy variables and indicator function


Let's familiariase ourselves with dummy variables. Dummy variable is a variable which can have either value 0 or 1. For the example above, the dummy variable can be used to indicate that diet of a mouse is `hf` instead of `chow`. Thus, let's create a dummy variable so that it takes a value $1$ if diet is `hf` and value $0$ if diet is `chow`.

In mathematical formulations, a useful way of reminding ourselves that a predictor is a dummy variable is to use an indicator function. Indicator function $\mathbf{1}(x)$ takes value $1$ if the condition `x` is true, and value $0$ otherwise. The notation of indicator variable nicely reminds us that it will give value $1$ or $0$.

Indicator function is
\[
\mathbf{1}(x) = \begin{cases}
      1 & \text{if condition $x$ is true}\\
      0 & \text{otherwise}
\end{cases}
\]

For example we may write
\[
Y_i = \beta_0 + \beta_1 \mathbf{1}(X_i = \texttt{hf}) + \varepsilon_i
\]
where $Y_i$ and $x_i$ stand for values of `Bodyweight` and `Diet` respectively, and  $i = 1,\dots,n$ and $\varepsilon_i \sim N(0,\sigma^2)$. In the above, the $\mathbf{1}(X_i = \texttt{hf})$ gives a value $1$ if `Diet` is `hf`. Otherwise it gives $0$.

Thus, the expected value of $Y_i$ is $\beta_0$ if diet is `chow`. If the diet is `hf`, the expected values of $Y_i$ is $\beta_0+\beta_1$.


\newpage
## 4. Basics of linear algebra
<!-- TODO: take something from labs-material -->

Linear algebra is a branch of mathematics where instead of numbers (aka scalars) we operate on vectors and matrices. Linear algebra is a useful tool for solving systems of linear equations, and thus suitable for handling mathematics behind linear regression. Next, we go through some of the key concepts so that we can move forward to understand how linear regression can be solved.

#### Transpose

Transpose of matrix $\mathbf{A}$ is denoted as $\mathbf{A}^T$
\[
\mathbf{A}=\begin{pmatrix}
  1 & 2\\
  3 & 4 \\
  5 & 6
\end{pmatrix}
\implies \mathbf{A}^T = \begin{pmatrix}
  1 & 3 & 5\\
  2 & 4 & 6\\
\end{pmatrix}
\]
Above matrix $A$ is $n$ times $p$ matrix, where $n=2, p=3$.

```{r}
A <- matrix(c(1,2,
              3,4,
              5,6),nrow=3,ncol=2,byrow=T)
A
t(A)
```

### Vector

A matrix with only one column is called a vector.
\[
d=\begin{pmatrix}
  1 \\
  3 \\
  5 
\end{pmatrix}
\]

A transpose of a vector is a row vector.
\[
d^T=\begin{bmatrix}
  1  & 3 & 5\\
\end{bmatrix}
\]

#### Cross product

A cross product (aka matrix multiplication) of row vector with a column vector is 
\[
\begin{bmatrix}
  1  & 2 & 3\\
\end{bmatrix}
\begin{pmatrix}
  4 \\
  5 \\
  6 
\end{pmatrix}
= \begin{bmatrix}
  1\cdot 4  + 2\cdot 5 + 3\cdot 6\\
\end{bmatrix}
= \begin{bmatrix}
  32\\
\end{bmatrix}
\]

```{r}
d <- c(4,5,6)
d
t(d)
t(t(d))
c(1,2,3) %*% d
```

#### Identity matrix

Identity matrix is $\mathbf{I}$ has 1 on diagonal and 0 elsewhere. E.g. 
\[
\mathbf{I}_3=\begin{pmatrix}
  1 & 0 & 0\\
  0 & 1 & 0\\
  0 & 0 & 1\\
\end{pmatrix}
\]
```{r}
diag(3)
```

#### Square matrix and inverse of a matrix

If matrix has the same number of rows and columns, it is called a square matrix.

If a square matrix $\mathbf{B}$ has determinant, which is not zero, then $\mathbf{B}$ is invertible, which means that it is possible to calculate an inverse of matrix $\mathbf{B}$ that is $\mathbf{B^{-1}}$.

```{r}
# Let's create a square matrix
B <- t(A) %*% A
B
det(B) #determinant is not zero => invertible
solve(B)
```


For invertible matrix $\mathbf{B}$ it follows that $\mathbf{B} \mathbf{B}^{-1} = \mathbf{I}$ and also $\mathbf{B}^{-1} \mathbf{B} = \mathbf{I}$.

### Solving systems of equations using linear algebra

Matrices can be used to represent a systems of equations. To understand the basic concepts, let's have a look at following system of equations:
\[
\begin{aligned}
a + b + c &= 6\\
3a - 2b + c &= 2\\
2a + b  - c &= 1
\end{aligned}
\]

\[
\,
\begin{pmatrix}
1&1&1\\
3&-2&1\\
2&1&-1
\end{pmatrix}
\begin{pmatrix}
a\\
b\\
c
\end{pmatrix} =
\begin{pmatrix}
6\\
2\\
1
\end{pmatrix}
\]

#### {-}

Taking the inverse of square matrix and multiplying the equation from the left hand side with that solves the system.
\[
\implies 
\begin{pmatrix}
a\\
b\\
c
\end{pmatrix} =
\begin{pmatrix}
1&1&1\\
3&-2&1\\
2&1&-1
\end{pmatrix}^{-1}
\begin{pmatrix}
6\\
2\\
1
\end{pmatrix}
\]
The same as above with R
```{r}
A <- matrix(c(1, 1, 1,
              3,-2, 1,
              2, 1,-1),nrow=3,ncol=3,byrow=T)
d <- c(6,2,1)
solve(A) %*% d
```


## 5. Linear models formulated with linear algebra
<!-- TODO: take something from labs-material -->

Let's consider the previous example of two mice populations in Chapter 3. We have already seen how linear regression connects nicely with t-tests. We will now go through how it can be formulated using linear algebra.

Let $x_i = \mathbf{1}(\texttt{Diet = hf})$ which means that $x_i=1$ if diet is `hf`, and $x_i = 0$ if diet is `chow`.

Lets define notation of matrices and vectors
\[
\mathbf{Y} = \begin{pmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_n
\end{pmatrix}
,
\mathbf{X} = \begin{pmatrix}
1&x_1\\
1&x_2\\
\vdots\\
1&x_n
\end{pmatrix}
,
\boldsymbol{\beta} = \begin{pmatrix}
\beta_0\\
\beta_1
\end{pmatrix} \mbox{ and }
\boldsymbol{\varepsilon} = \begin{pmatrix}
\varepsilon_1\\
\varepsilon_2\\
\vdots\\
\varepsilon_n
\end{pmatrix}
\]

So we can write linear regression using the notation above as
\[
\,
\begin{pmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_n
\end{pmatrix} = 
\begin{pmatrix}
1&x_1\\
1&x_2\\
\vdots\\
1&x_n
\end{pmatrix}
\begin{pmatrix}
\beta_0\\
\beta_1
\end{pmatrix} +
\begin{pmatrix}
\varepsilon_1\\
\varepsilon_2\\
\vdots\\
\varepsilon_n
\end{pmatrix}
\]

and we can use matrix notation, so the above can be written as

\[
\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}
\]

#### {-}

\[
\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}
\]

The $\mathbf{X}$ is what we call a design matrix. We can show design matrix in R from an object of `lm` by:
```{r, eval=FALSE}
model.matrix(~ Diet, data=dat)
# or
model.matrix(m1)
```


<!-- ## 4. Linear hypothesis testing with L-matrices -->
<!-- TODO: Take inspiration to this content from the Tilastotieteen peruskurssi -->

## 6. Contrasts
<!-- TODO: take something from labs-material -->
<!-- See http://www.bioconductor.org/packages/release/bioc/vignettes/limma/inst/doc/usersguide.pdf pdf-page 44. -->

As a running example to learn about more complex linear models, we will be using a dataset which compares the different frictional coefficients on the different legs of a spider. Specifically, we will be determining whether more friction comes from a pushing or pulling motion of the leg. 
<!-- The original paper from which the data was provided is: -->

<!-- Jonas O. Wolff  & Stanislav N. Gorb, [Radial arrangement of Janus-like setae permits friction control in spiders](http://dx.doi.org/10.1038/srep01101), Scientific Reports, 22 January 2013. -->

<!-- The abstract of the paper says,  -->

<!-- > The hunting spider Cupiennius salei (Arachnida, Ctenidae) possesses hairy attachment pads (claw tufts) at its distal legs, consisting of directional branched setae... Friction of claw tufts on smooth glass was measured to reveal the functional effect of seta arrangement within the pad. -->

<!-- [Figure 1](http://www.nature.com/articles/srep01101/figures/1)  -->
<!-- includes some pretty cool electron microscope images of the tufts. We are interested in the comparisons in  -->
<!-- [Figure 4](http://www.nature.com/articles/srep01101/figures/4),  -->
<!-- where the pulling and pushing motions are compared for different leg pairs (for a diagram of pushing and pulling see the top of  -->
<!-- [Figure 3](http://www.nature.com/articles/srep01101/figures/3)).  -->

<!-- We include the data in our dagdata package and can download it from [here](https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/spider_wolff_gorb_2013.csv). -->

```{r}
spider <- read.csv("spider_wolff_gorb_2013.csv", skip=1)
```


#### Initial visual inspection of the data

Each measurement comes from one of our legs while it is either pushing or pulling. So we have two variables. In RStudio we may view the data as

```{r, eval=F}
View(spider)
```
or just print it to console using `spider` or have a glimpse of first 6 rows
```{r}
head(spider)
```

#### {-}

We can make a boxplot
```{r spide_data, fig.cap="Comparison of friction coefficients of spiders' different leg pairs. The friction coefficient is calculated as the ratio of two forces (see paper Methods) so it is unitless."}
boxplot(spider$friction ~ spider$type * spider$leg, 
        col=c("grey90","grey40"), las=2, 
        main="Comparison of friction coefficients of different leg pairs")
```


<!-- What we can immediately see are two trends:  -->

<!-- * The pulling motion has higher friction than the pushing motion. -->
<!-- * The leg pairs to the back of the spider (L4 being the last) have higher pulling friction. -->

<!-- Another thing to notice is that the groups have different spread around their average, what we call *within-group variance*. This is somewhat of a problem for the kinds of linear models we will explore below, since we will be assuming that around the population average values, the errors $\varepsilon_i$ are distributed identically, meaning the same variance within each group. The consequence of ignoring the different variances for the different groups is that comparisons between those groups with small variances will be overly "conservative" (because the overall estimate of variance is larger than an estimate for just these groups), and comparisons between those groups with large variances will be overly confident. If the spread is related to the range of friction, such that groups with large friction values also have larger spread, a possibility is to transform the data with a function such as the `log` or `sqrt`. This looks like it could be useful here, since three of the four push groups (L1, L2, L3) have the smallest friction values and also the smallest spread. -->

<!-- Some alternative tests for comparing groups without transforming the values first include: t-tests without the equal variance assumption using a "Welch" or "Satterthwaite approximation", or the Wilcoxon rank sum test mentioned previously. However here, for simplicity of illustration, we will fit a model that assumes equal variance and shows the different kinds of linear model designs using this dataset, setting aside the issue of different within-group variances. -->

<!-- #### A linear model with one variable -->

<!-- To remind ourselves how the simple two-group linear model looks, we will subset the data to include only the L1 leg pair, and run `lm`: -->

<!-- ```{r} -->
<!-- spider.sub <- spider[spider$leg == "L1",] -->
<!-- fit <- lm(friction ~ type, data=spider.sub) -->
<!-- summary(fit) -->
<!-- (coefs <- coef(fit)) -->
<!-- ``` -->

<!-- These two estimated coefficients are the mean of the pull observations (the first estimated coefficient) and the difference between the means of the two groups (the second coefficient). We can show this with R code: -->

<!-- ```{r} -->
<!-- s <- split(spider.sub$friction, spider.sub$type) -->
<!-- mean(s[["pull"]]) -->
<!-- mean(s[["push"]]) - mean(s[["pull"]]) -->
<!-- ``` -->

<!-- We can form the design matrix, which was used inside `lm`: -->

<!-- ```{r} -->
<!-- X <- model.matrix(~ type, data=spider.sub) -->
<!-- colnames(X) -->
<!-- head(X) -->
<!-- tail(X) -->
<!-- ``` -->

<!-- Now we'll make a plot of the $\mathbf{X}$ matrix by putting a black block for the 1's and a white block for the 0's. This plot will be more interesting for the linear models later on in this script. Along the y-axis is the sample number (the row number of the `data`) and along the x-axis is the column of the design matrix $\mathbf{X}$. If you have installed the *rafalib* library, you can make this plot with the `imagemat` function: -->

<!-- ```{r model_matrix_image, fig.cap="Model matrix for linear model with one variable."} -->
<!-- library(rafalib) -->
<!-- imagemat(X, main="Model matrix for linear model with one variable") -->
<!-- ``` -->

<!-- #### Examining the estimated coefficients -->

<!-- Now we show the coefficient estimates from the linear model in a diagram with arrows (code not shown). -->

<!-- ```{r spider_main_coef, fig.cap="Diagram of the estimated coefficients in the linear model. The green arrow indicates the Intercept term, which goes from zero to the mean of the reference group (here the 'pull' samples). The orange arrow indicates the difference between the push group and the pull group, which is negative in this example. The circles show the individual samples, jittered horizontally to avoid overplotting.",echo=FALSE} -->
<!-- set.seed(1) #same jitter in stripchart -->
<!-- stripchart(split(spider.sub$friction, spider.sub$type),  -->
<!--            vertical=TRUE, pch=1, method="jitter", las=2, xlim=c(0,3), ylim=c(0,2)) -->
<!-- a <- -0.25 -->
<!-- lgth <- .1 -->
<!-- library(RColorBrewer) -->
<!-- cols <- brewer.pal(3,"Dark2") -->
<!-- abline(h=0) -->
<!-- arrows(1+a,0,1+a,coefs[1],lwd=3,col=cols[1],length=lgth) -->
<!-- abline(h=coefs[1],col=cols[1]) -->
<!-- arrows(2+a,coefs[1],2+a,coefs[1]+coefs[2],lwd=3,col=cols[2],length=lgth) -->
<!-- abline(h=coefs[1]+coefs[2],col=cols[2]) -->
<!-- legend("right",names(coefs),fill=cols,cex=.75,bg="white") -->
<!-- ``` -->

#### A linear model with two variables

<!-- Now we'll continue and examine the full dataset, including the observations from all leg pairs.  -->
In order to model both the leg pair differences (L1, L2, L3, L4) and the push vs. pull difference, we need to include both terms in the R formula. Let's see what kind of design matrix will be formed with two variables in the formula:

```{r model_matrix_image2, fig.cap="Image of the model matrix for a formula with type + leg"}
X <- model.matrix(~ type + leg, data=spider)
```
```{r, eval=F}
View(X)
```

<!-- The first column is the intercept, and so it has 1's for all samples. The second column has 1's for the push samples, and we can see that there are four groups of them. Finally, the third, fourth and fifth columns have 1's for the L2, L3 and L4 samples. The L1 samples do not have a column, because *L1* is the reference level for `leg`. Similarly, there is no *pull* column, because *pull* is the reference level for the `type` variable. -->

To estimate coefficients for this model, we use `lm` with the formula `~ type + leg`. We'll save the linear model to `fitTL` standing for a *fit* with *Type* and *Leg*.

```{r}
fitTL <- lm(friction ~ type + leg, data=spider)
summary(fitTL)
coefs <- coef(fitTL)
coefs
```

R uses the name `Coefficients` to denote the component containing the least squares **estimates**. It is important to remember that the coefficients are parameters that we do not observe, but only estimate.

#### Mathematical representation

The model we are fitting above can be written as

\[
Y_i = \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + \beta_3 x_{i,3} + \beta_4 x_{i,4} + \varepsilon_i, i=1,\dots,n
\]
or consider the indicator variable style
\[
\texttt{friction} = \beta_0 + \beta_1\mathbf{1}(\texttt{type=push}) + \beta_2\mathbf{1}(\texttt{leg=L2}) + \beta_3 \mathbf{1}(\texttt{leg=L3}) + \beta_4 \mathbf{1}(\texttt{leg=L4}) + \varepsilon_i, ~i=1,\dots,n
\]

<!-- with the $x$ all indicator variables denoting push or pull and which leg. For example, a push on leg 3 will have $x_{i,1}$ and $x_{i,3}$ equal to 1 and the rest would be 0. Throughout this section we will refer to the $\beta$ s with the effects they represent. For example we call $\beta_0$ the intercept, $\beta_1$ the pull effect, $\beta_2$ the L2 effect, etc. We do not observe the coefficients, e.g.  $\beta_1$, directly, but estimate them with, e.g. $\hat{\beta}_4$. -->

We can now form the matrix $\mathbf{X}$ depicted above and obtain the least square estimates with:

$$ \hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y} $$

```{r}
Y <- spider$friction
X <- model.matrix(~ type + leg, data=spider)
beta.hat <- solve(t(X) %*% X) %*% t(X) %*% Y
t(beta.hat)
```

We can see that these values agree with the output of `lm`.

#### Examining the estimated coefficients

We can make the same plot as before, with arrows for each of the estimated coefficients in the model (code not shown). 

```{r spider_interactions, fig.cap="Diagram of the estimated coefficients in the linear model.",echo=FALSE}
spider$group <- factor(paste0(spider$leg, spider$type))
stripchart(split(spider$friction, spider$group), 
           vertical=TRUE, pch=1, method="jitter", las=2, xlim=c(0,11), ylim=c(0,2))
a <- -0.25
lgth <- .1
library(RColorBrewer)
cols <- RColorBrewer::brewer.pal(5,"Dark2")
abline(h=0)
arrows(1+a,0,1+a,coefs[1],lwd=3,col=cols[1],length=lgth)
abline(h=coefs[1],col=cols[1])
arrows(3+a,coefs[1],3+a,coefs[1]+coefs[3],lwd=3,col=cols[3],length=lgth)
arrows(5+a,coefs[1],5+a,coefs[1]+coefs[4],lwd=3,col=cols[4],length=lgth)
arrows(7+a,coefs[1],7+a,coefs[1]+coefs[5],lwd=3,col=cols[5],length=lgth)
arrows(2+a,coefs[1],2+a,coefs[1]+coefs[2],lwd=3,col=cols[2],length=lgth)
segments(3+a,coefs[1]+coefs[3],4+a,coefs[1]+coefs[3],lwd=3,col=cols[3])
arrows(4+a,coefs[1]+coefs[3],4+a,coefs[1]+coefs[3]+coefs[2],lwd=3,col=cols[2],length=lgth)
segments(5+a,coefs[1]+coefs[4],6+a,coefs[1]+coefs[4],lwd=3,col=cols[4])
arrows(6+a,coefs[1]+coefs[4],6+a,coefs[1]+coefs[4]+coefs[2],lwd=3,col=cols[2],length=lgth)
segments(7+a,coefs[1]+coefs[5],8+a,coefs[1]+coefs[5],lwd=3,col=cols[5])
arrows(8+a,coefs[1]+coefs[5],8+a,coefs[1]+coefs[5]+coefs[2],lwd=3,col=cols[2],length=lgth)
legend("right",names(coefs),fill=cols,cex=.75,bg="white")
```

In this case, the fitted means for each group, derived from the fitted coefficients, do not line up with those we obtain from simply taking the average from each of the eight possible groups. The reason is that our model uses five coefficients, instead of eight. We are **assuming** that the effects are additive. 

However, this particular dataset is better described with a model including interactions. We omit going into details with that due to lack of time.
 
#### Contrasting coefficients

Sometimes, the comparison we are interested in is represented directly by a single coefficient in the model, such as the push vs. pull difference, which was `coefs[2]` above. However, sometimes, we want to make a comparison which is not a single coefficient, but a combination of coefficients, which is called a _contrast_. To introduce the concept of _contrasts_, first consider the comparisons which we can read off from the linear model summary:

```{r}
coef(fitTL)
```

Here we have the intercept estimate, the push vs. pull estimated effect across all leg pairs, and the estimates for the L2 vs. L1 effect, the L3 vs. L1 effect, and the L4 vs. L1 effect. What if we want to compare two groups and one of those groups is not L1? The solution to this question is to use *contrasts*. 

#### {-}

A *contrast* is a combination of estimated coefficient: $\mathbf{c^\top} \hat{\boldsymbol{\beta}}$, where $\mathbf{c}$ is a column vector with as many rows as the number of coefficients in the linear model. If $\mathbf{c}$ has a 0 for one or more of its rows, then the corresponding estimated coefficients in $\hat{\boldsymbol{\beta}}$ are not involved in the contrast.

If we want to compare leg pairs L3 and L2, this is equivalent to contrasting two coefficients from the linear model because, in this contrast, the comparison to the reference level *L1* cancels out:

$$ (\mbox{L3} - \mbox{L1}) - (\mbox{L2} - \mbox{L1}) = \mbox{L3} - \mbox{L2 }$$

#### {-}

An easy way to make these contrasts of two groups is to use the `contrast` function from the *contrast* package. We just need to specify which groups we want to compare. We have to pick one of *pull* or *push* types, although the answer will not differ, as we will see below.

```{r,message=FALSE,warning=FALSE}
library(contrast) #Available from CRAN
L3vsL2 <- contrast(fitTL,list(leg="L3",type="pull"),list(leg="L2",type="pull"))
L3vsL2
```

The first column `Contrast` gives the L3 vs. L2 estimate from the model we fit above.

#### {-}

We can show that the least squares estimates of a linear combination of coefficients is the same linear combination of the estimates. 
Therefore, the effect size estimate is just the difference between two estimated coefficients. The contrast vector used by `contrast` is stored as a variable called `X` within the resulting object (not to be confused with our original $\mathbf{X}$, the design matrix).

```{r}
coefs[4] - coefs[3]
(cT <- L3vsL2$X)
cT %*% coefs
```

### Remarks

There will be an update on exercises:

 - Topic of confounding (the very last) will be removed. I try to figure a task about contrasts.